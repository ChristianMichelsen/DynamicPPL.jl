# The Basics

DynamicPPL is a probabilistic programming language embedded in Julia. You can think of it as the backend that powers Turing, doing things like:
* Converting your readable Turing code into runnable Julia code with the `@model` macro.
* Providing information to samplers about a model.
* Keeping track of random variables while sampling.
* Calculating the log-pdf for possible events.

In this tutorial, we'll see how to do all of this. Let's start by creating a basic model.
```@example
using DynamicPPL, Distributions, Random
rng = Xoshiro(1776);  # Set seed for reproducibility
```

This model assumes that our sample follows a normal distribution with a standard deviation of 1 and an unknown mean.
```@example
@model function demo(x)
   μ ~ Normal(0, 1)  # Our prior is a standard normal distribution
   x .~ Normal(μ, 1)
   return nothing
end
```

We instantiate the model by calling it on a dataset.
```@example
dataset = [-1, 1, 1]
model = demo(dataset)
```

In our case, we can calculate the posterior analytically. Let's do that so we can compare results later:

```@example 
posterior = Normal(
	mean(vcat(dataset, 0)), 
	1 / sqrt(length(dataset) + 1)
)
```


### Evaluating a model

What *is* a DynamicPPL model? This is an important question, because the way DynamicPPL treats models can be different from how you'd think of them.

DynamicPPL is *procedural*: everything is a series of instructions (a procedure). This procedure modifies or returns `AbstractVarInfo` objects, which hold samples taken from a probability distribution. As an example, a `SimpleVarInfo` has two fields, a `NamedTuple` of variables and a log-probability:


```@example
x0 = SimpleVarInfo((μ=0,), logpdf(posterior, 0))
```

The procedural approach is different from the object-oriented approach. We don't try to reason through a model by thinking of each `~` as a property of a model object, or think of the model as a coherent "Whole" to be queried. A `~` is an *action*.

We can see this by taking a look at what's inside our model. A `Model` is pretty much just a wrapper around a function `f`, with hardly any methods or fields:


```@example
fieldnames(DynamicPPL.Model)
```

Procedural programming is also very different from functional programming, where we might think of a model as a pure mathematical function, like a probability density. Each `~` statement in DynamicPPL is not a pure function: lines can behave differently depending on the model's state. This state is contained in the `Context` and `Sampler` arguments we pass to a model.

We can execute a model using `DynamicPPL.evaluate!!`. This will always return two values: the return value for the function (in our case `nothing`) and a `VarInfo`. (It will also modify `VarInfo` if `VarInfo` is mutable.)

Let's run through the most important DynamicPPL `Context`s. First we have the `SamplingContext`, which draws a new `VarInfo` using a given sampler. By default, this draws a sample from the prior.

```@example
# Here, our sampler is SampleFromPrior()
_, x1 = DynamicPPL.evaluate!!(model, x0, SamplingContext())
```

On the other hand, if we call a model with a `LikelihoodContext` and a preexisting `VarInfo`, the model evaluates the likelihood function (ignoring the prior) and inserts it into the `logp` field, leaving the sample unchanged:
```@example
_, x2 = DynamicPPL.evaluate!!(model, deepcopy(x1), LikelihoodContext())
```

And the value of `logp` for `x1` is equal to the likelihood plus the prior:

```@example
_, x3 = DynamicPPL.evaluate!!(model, deepcopy(x1), PriorContext())
getlogp(x1) ≈ getlogp(x2) + getlogp(x3)
```

For convenience, we also provide the functions `logprior`, `loglikelihood`, and `logjoint` to calculate probabilities for a `VarInfo`:


```@example
logjoint(model, x1) ≈ loglikelihood(model, x1) + logprior(model, x1)
```


Some contexts can be nested. For instance, `SamplingContext` can be nested with a `PriorContext` to insert the log-prior, rather than the log-posterior into `logp`.


```@example
_, x4 = DynamicPPL.evaluate!!(model, x1, SamplingContext())
```

By default, we evaluate the log-posterior. This can be specified explicitly using `DefaultContext()`.

```@example
_, x5 = DynamicPPL.evaluate!!(model, deepcopy(x1), DefaultContext())
x5 == x1
```


### Example: A simple sampler

Let's create a Metropolis-Hastings Sampler to see how this works:


```@example
function metropolis(
	rng,
	init::DynamicPPL.AbstractVarInfo, 
	model::DynamicPPL.Model, 
	proposal::Distribution, 
	n_steps::Int
)
	# preallocate a vector to hold the VarInfo samples.
	samples = Vector{typeof(init)}(undef, n_steps)
	current = init

	for step in 1:n_steps
		# Draw a new proposed point.
		proposal = SimpleVarInfo(
			map(current.values) do x
				x + rand(rng, proposal)
			end
		)

		# Evaluate the log-probability of the proposed point.
		_, proposal = DynamicPPL.evaluate!!(model, proposal, DefaultContext())
		
		# Decide if we accept the proposal.
		log_p = getlogp(current) - getlogp(proposal)
		accept = randexp(rng) > log_p
		
		# Add the point to the samples.
		samples[step] = accept ? proposal : current
		current = samples[step]
	end
	return samples
end
```

```
function mh_sampling(rng, model::DynamicPPL.Model, proposal::Distribution, n_steps::Int)
	# First we create a SimpleVarInfo by sampling from the prior, to initialize the model. 
	# For convenience, SimpleVarInfo(model) samples from the prior by default.
	init = SimpleVarInfo(model)
	# Now we use a function barrier to let Julia infer the correct types.
	return metropolis(rng, init, model, kernel, n_steps)
end
```

```@example
samples = sample(rng, model, Normal(0, .05), 1_000_000)
```

And now we can see we have the right mean!

```@example
means = getindex.(samples, (@varname(μ),))
mean(means)
```
